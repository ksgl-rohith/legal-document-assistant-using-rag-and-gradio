{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "_l2pytnvNwjB",
        "outputId": "efc74cfe-0734-4fe7-cfe5-30a5a30264dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://008f56c481bf29cdeb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://008f56c481bf29cdeb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q gradio faiss-cpu sentence-transformers pandas requests\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import gradio as gr\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ---------- NVIDIA API ----------\n",
        "NVIDIA_API_KEY = \"NVIDIA_API_KEY\" \n",
        "NVIDIA_CHAT_URL = \"CHAT_API_ENDPOINT\" \n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {NVIDIA_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "def nvidia_llm(query, context=\"\"):\n",
        "    try:\n",
        "        payload = {\n",
        "            \"model\": \"meta/llama-3.1-8b-instruct\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful eCommerce assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion:\\n{query}\"}\n",
        "            ],\n",
        "            \"temperature\": 0.3\n",
        "        }\n",
        "        resp = requests.post(NVIDIA_CHAT_URL, headers=headers, json=payload)\n",
        "        return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è LLM call failed: {e}\"\n",
        "\n",
        "# ---------- Embeddings & FAISS ----------\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "dimension = 384\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "products = []  # Stores product descriptions\n",
        "\n",
        "# ---------- Auto-update Knowledge Base ----------\n",
        "def update_products(csv_file):\n",
        "    global products, index\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file.name)\n",
        "        product_texts = (df[\"product_name\"].astype(str) + \" \" +\n",
        "                         df[\"description\"].astype(str) + \" \" +\n",
        "                         df[\"category\"].astype(str)).tolist()\n",
        "        embeddings = embedder.encode(product_texts)\n",
        "        index.reset()\n",
        "        index.add(np.array(embeddings, dtype=np.float32))\n",
        "        products = product_texts\n",
        "        return f\"‚úÖ Loaded {len(products)} products into knowledge base.\"\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è Failed to load products: {e}\"\n",
        "\n",
        "# ---------- RAG Query ----------\n",
        "def rag_query(query):\n",
        "    if len(products) == 0:\n",
        "        return \"‚ö†Ô∏è No product data loaded. Please upload product CSV.\"\n",
        "\n",
        "    vec = embedder.encode([query])\n",
        "    D, I = index.search(np.array(vec, dtype=np.float32), k=3)\n",
        "    if len(I[0]) == 0 or I[0][0] == -1:\n",
        "        return \"‚ö†Ô∏è No relevant products found.\"\n",
        "\n",
        "    context = \"\\n\".join([products[i] for i in I[0]])\n",
        "    answer = nvidia_llm(query, context)\n",
        "    return answer\n",
        "\n",
        "# ---------- Gradio UI ----------\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üõí RAG eCommerce Assistant (NVIDIA + FAISS)\")\n",
        "\n",
        "    with gr.Tab(\"üìÇ Upload Products\"):\n",
        "        csv_upload = gr.File(label=\"Upload Product CSV\", file_types=[\".csv\"])\n",
        "        upload_status = gr.Textbox(label=\"Status\")\n",
        "        csv_upload.upload(update_products, csv_upload, upload_status)\n",
        "\n",
        "    with gr.Tab(\"üí¨ Chatbot\"):\n",
        "        query = gr.Textbox(label=\"Ask about products, recommendations, or suggestions\")\n",
        "        answer = gr.Textbox(label=\"LLM Response\", lines=15)\n",
        "        ask_btn = gr.Button(\"Ask\")\n",
        "        ask_btn.click(rag_query, query, answer)\n",
        "\n",
        "    with gr.Tab(\"üìä Embeddings Viewer\"):\n",
        "        emb_inp = gr.Textbox(label=\"Enter text to see embeddings\")\n",
        "        emb_btn = gr.Button(\"Generate Embeddings\")\n",
        "        emb_out = gr.Textbox(label=\"Embedding Vector\", lines=15)\n",
        "        def show_embeddings(text):\n",
        "            vec = embedder.encode([text])[0]\n",
        "            return f\"üî¢ Embedding vector (length {len(vec)}):\\n\\n{vec}\"\n",
        "        emb_btn.click(show_embeddings, emb_inp, emb_out)\n",
        "\n",
        "demo.launch(inline=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzH0Dg4KNx6P"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
